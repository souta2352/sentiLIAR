{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1feNbzyRv6uShkbQaBCLjucyYoFaVmq5z",
      "authorship_tag": "ABX9TyOqtiy5KAUxRFYCnSKZFBJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souta2352/sentiLIAR/blob/main/1031_sentimentalLIAR_ipynb_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qYcWqmWYMUwb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpdnVTyJGcRs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv4YdggxhXSK"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/SentimentalLIAR/SentimentalLIAR_master_train_final.csv\")\n",
        "df_test=pd.read_csv(\"/content/drive/MyDrive/SentimentalLIAR/SentimentalLIAR_master_test_final.csv\")\n",
        "df_valid=pd.read_csv(\"/content/drive/MyDrive/SentimentalLIAR/SentimentalLIAR_master_valid_final.csv\")\n",
        "print(\"before truncating size of data is :\", df.shape, df_test.shape,df_valid.shape)\n",
        "df=df[:10232]\n",
        "df_test=df_test[:1264]\n",
        "df_valid=df_valid[:1280]\n",
        "print(\"size of data is :\", df.shape, df_test.shape, df_valid.shape)\n",
        "\n",
        "\n",
        "#check if any null values are present\n",
        "print(\"Any null in Subject? \",df['subject'].isnull().values.any())\n",
        "print(\"Any null in Speaker? \",df['speaker'].isnull().values.any())\n",
        "print(\"Any null in speaker_job? \",df['speaker_job'].isnull().values.any())\n",
        "print(\"Any null in Party? \",df['party_affiliation'].isnull().values.any())\n",
        "print(\"Any null in Context? \",df['context'].isnull().values.any())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ttnf--6CwEg",
        "outputId": "7b5431ae-a00b-4043-e200-32d0dad1ed68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before truncating size of data is : (10236, 31) (1267, 30) (1283, 31)\n",
            "size of data is : (10232, 31) (1264, 30) (1280, 31)\n",
            "Any null in Subject?  False\n",
            "Any null in Speaker?  False\n",
            "Any null in speaker_job?  True\n",
            "Any null in Party?  False\n",
            "Any null in Context?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"data_statement\"]=df[\"statement\"]\n",
        "df['data_subject']=df['subject'].astype(str)\n",
        "df['data_speaker_job']=df['speaker_job'].astype(str)\n",
        "df['data_party_affiliation']=df['party_affiliation'].astype(str)\n",
        "df['data_context']=df['context'].astype(str)\n",
        "# df['sentiment_code']=df['sentiment_code'].astype(str)\n",
        "\n",
        "\n",
        "df_test[\"data_statement\"]=df_test[\"statement\"]\n",
        "df_test['data_subject']=df_test['subject'].astype(str)\n",
        "df_test['data_speaker_job']=df_test['speaker_job'].astype(str)\n",
        "df_test['data_party_affiliation']=df_test['party_affiliation'].astype(str)\n",
        "df_test['data_context']=df_test['context'].astype(str)\n",
        "# df_test['sentiment_code']=df_test['sentiment_code'].astype(str)\n",
        "\n",
        "\n",
        "df_valid[\"data_statement\"]=df_valid[\"statement\"]\n",
        "df_valid['data_subject']=df_valid['subject'].astype(str)\n",
        "df_valid['data_speaker_job']=df_valid['speaker_job'].astype(str)\n",
        "df_valid['data_party_affiliation']=df_valid['party_affiliation'].astype(str)\n",
        "df_valid['data_context']=df_valid['context'].astype(str)\n",
        "# df_valid['sentiment_code']=df_valid['sentiment_code'].astype(str)"
      ],
      "metadata": {
        "id": "Aag2DRuqDJOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenate emotion, speakers' credit and sentiment score togehter\n",
        "df['emotion']=\"[\"+df['anger'].astype(str)+\",\"+df['disgust'].astype(str)+\",\"\\\n",
        "+df['fear'].astype(str)+\",\"+df['joy'].astype(str)+\",\"+df['sad'].astype(str)+\",\"+\\\n",
        "df[\"barely_true_counts\"].astype(str) +\",\"+ df[\"false_counts\"].astype(str)  +\",\"+\\\n",
        "df[\"half_true_counts\"].astype(str) + \",\"+df[\"mostly_true_counts\"].astype(str) +\",\"+ \\\n",
        "df[\"pants_on_fire_counts\"].astype(str)+\",\"+df[\"sentiment_score\"].astype(str)+\"]\"\n",
        "\n",
        "\n",
        "df_test['emotion']=\"[\"+df_test['anger'].astype(str)+\",\"+df_test['disgust'].astype(str)+\",\"\\\n",
        "+df_test['fear'].astype(str)+\",\"+df_test['joy'].astype(str)+\",\"+df_test['sad'].astype(str)\\\n",
        "+ \",\"+df_test[\"barely_true_counts\"].astype(str) + \",\"+ df_test[\"false_counts\"].astype(str) \\\n",
        "+\",\"+ df_test[\"half_true_counts\"].astype(str) +\",\"+ df_test[\"mostly_true_counts\"].astype(str)\\\n",
        "+\",\"+ df_test[\"pants_on_fire_counts\"].astype(str)+\",\"+df_test[\"sentiment_score\"].astype(str)+\"]\"\n",
        "\n",
        "\n",
        "df_valid['emotion']=\"[\"+df_valid['anger'].astype(str)+\",\"+df_valid['disgust'].astype(str)+\",\"\\\n",
        "+df_valid['fear'].astype(str)+\",\"+df_valid['joy'].astype(str)+\",\"+df_valid['sad'].astype(str)\\\n",
        "+ \",\"+df_valid[\"barely_true_counts\"].astype(str) + \",\"+ df_valid[\"false_counts\"].astype(str) \\\n",
        "+\",\"+ df_valid[\"half_true_counts\"].astype(str) +\",\"+ df_valid[\"mostly_true_counts\"].astype(str)\\\n",
        "+\",\"+ df_valid[\"pants_on_fire_counts\"].astype(str)+\",\"+df_valid[\"sentiment_score\"].astype(str)+\"]\"\n"
      ],
      "metadata": {
        "id": "uME_pQvgDNdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#target should be converted from string to the list\n",
        "import ast\n",
        "def convert_to_list(text):\n",
        "  return ast.literal_eval(text)"
      ],
      "metadata": {
        "id": "e4XwH_XLDQBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df[\"emotion\"])):\n",
        "  try:\n",
        "    df[\"emotion\"][i]=convert_to_list(df[\"emotion\"][i])\n",
        "  except:\n",
        "    print(i,\"====\",df[\"emotion\"][1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jgY6P4tDR4S",
        "outputId": "42e96367-d068-410c-a6a1-d61d52d9a8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-5b93fc2d151f>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"emotion\"][i]=convert_to_list(df[\"emotion\"][i])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df_test[\"emotion\"])):\n",
        "  try:\n",
        "    df_test[\"emotion\"][i]=convert_to_list(df_test[\"emotion\"][i])\n",
        "  except:\n",
        "    print(i,\"====\",df_test[\"emotion\"][i], type(df_test[\"emotion\"][i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krFf8ifkDZ41",
        "outputId": "63bb2398-66d0-4869-8510-54b224df3f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-72-14a884a6d24f>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_test[\"emotion\"][i]=convert_to_list(df_test[\"emotion\"][i])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df_valid[\"emotion\"])):\n",
        "  try:\n",
        "    df_valid[\"emotion\"][i]=convert_to_list(df_valid[\"emotion\"][i])\n",
        "  except:\n",
        "    print(i,\"====\",df_valid[\"emotion\"][i], type(df_valid[\"emotion\"][i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0cBqRKXDeGO",
        "outputId": "9b01bfdd-1b9d-42e8-a199-54c72e3c3579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-73-8d54630e9ad5>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_valid[\"emotion\"][i]=convert_to_list(df_valid[\"emotion\"][i])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['list']=df['list'].apply(convert_to_list)\n",
        "df_test['list']=df_test['list'].apply(convert_to_list)\n",
        "df_valid['list']=df_valid['list'].apply(convert_to_list)"
      ],
      "metadata": {
        "id": "D9kcVJpIDepH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "MAX_LEN = 300\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "\n",
        "tokenizer_statement = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model_statement = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer_subject = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model_subject = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer_context = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model_context = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer_speaker_job = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model_speaker_job = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer_party_affiliation = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model_party_affiliation = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "8KxHRF_zDhiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.data_statement = dataframe.data_statement\n",
        "        self.data_subject = dataframe.data_subject\n",
        "        self.data_context = dataframe.data_context\n",
        "        self.data_speaker_job = dataframe.data_speaker_job\n",
        "        self.data_party_affiliation = dataframe.data_party_affiliation\n",
        "\n",
        "        self.targets = self.data.list\n",
        "        self.max_len = max_len\n",
        "        # Add emotion list from dataframe\n",
        "        self.emotion = dataframe.emotion\n",
        "        self.dfID = dataframe.ID\n",
        "\n",
        "    def process_text(self, text):\n",
        "        return self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        pad_to_max_length=True,\n",
        "        return_token_type_ids=True,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text_statement = str(self.data_statement[index])\n",
        "        text_subject = str(self.data_subject[index])\n",
        "        text_context = str(self.data_context[index])\n",
        "        text_speaker_job = str(self.data_speaker_job[index])\n",
        "        text_party_affiliation = str(self.data_party_affiliation[index])\n",
        "\n",
        "        inputs_statement = self.process_text(text_statement)\n",
        "        inputs_subject = self.process_text(text_subject)\n",
        "        inputs_context = self.process_text(text_context)\n",
        "        inputs_speaker_job = self.process_text(text_speaker_job)\n",
        "        inputs_party_affiliation = self.process_text(text_party_affiliation)\n",
        "\n",
        "        ids_statement, mask_statement, token_type_ids_statement = (\n",
        "            inputs_statement['input_ids'],\n",
        "            inputs_statement['attention_mask'],\n",
        "            inputs_statement['token_type_ids']\n",
        "        )\n",
        "        ids_subject, mask_subject, token_type_ids_subject = (\n",
        "            inputs_subject['input_ids'],\n",
        "            inputs_subject['attention_mask'],\n",
        "            inputs_subject['token_type_ids']\n",
        "        )\n",
        "        ids_context, mask_context, token_type_ids_context = (\n",
        "            inputs_context['input_ids'],\n",
        "            inputs_context['attention_mask'],\n",
        "            inputs_context['token_type_ids']\n",
        "        )\n",
        "        ids_speaker_job, mask_speaker_job, token_type_ids_speaker_job = (\n",
        "            inputs_speaker_job['input_ids'],\n",
        "            inputs_speaker_job['attention_mask'],\n",
        "            inputs_speaker_job['token_type_ids']\n",
        "        )\n",
        "        ids_party_affiliation, mask_party_affiliation, token_type_ids_party_affiliation = (\n",
        "            inputs_party_affiliation['input_ids'],\n",
        "            inputs_party_affiliation['attention_mask'],\n",
        "            inputs_party_affiliation['token_type_ids']\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'ids_statement': torch.tensor(ids_statement, dtype=torch.long),\n",
        "            'mask_statement': torch.tensor(mask_statement, dtype=torch.long),\n",
        "            'token_type_ids_statement': torch.tensor(token_type_ids_statement, dtype=torch.long),\n",
        "            'ids_subject': torch.tensor(ids_subject, dtype=torch.long),\n",
        "            'mask_subject': torch.tensor(mask_subject, dtype=torch.long),\n",
        "            'token_type_ids_subject': torch.tensor(token_type_ids_subject, dtype=torch.long),\n",
        "            'ids_context': torch.tensor(ids_context, dtype=torch.long),\n",
        "            'mask_context': torch.tensor(mask_context, dtype=torch.long),\n",
        "            'token_type_ids_context': torch.tensor(token_type_ids_context, dtype=torch.long),\n",
        "            'ids_speaker_job': torch.tensor(ids_speaker_job, dtype=torch.long),\n",
        "            'mask_speaker_job': torch.tensor(mask_speaker_job, dtype=torch.long),\n",
        "            'token_type_ids_speaker_job': torch.tensor(token_type_ids_speaker_job, dtype=torch.long),\n",
        "            'ids_party_affiliation': torch.tensor(ids_party_affiliation, dtype=torch.long),\n",
        "            'mask_party_affiliation': torch.tensor(mask_party_affiliation, dtype=torch.long),\n",
        "            'token_type_ids_party_affiliation': torch.tensor(token_type_ids_party_affiliation, dtype=torch.long),\n",
        "            'emotion':torch.tensor(self.emotion[index], dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "BvlcSMp4DjD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_size = 1\n",
        "train_dataset=df.sample(frac=train_size,random_state=200).reset_index(drop=True)\n",
        "test_dataset=df_test.sample(frac=train_size,random_state=200).reset_index(drop=True)\n",
        "valid_dataset=df_valid.sample(frac=1,random_state=200).reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "print(\"VALID Dataset: {}\".format(valid_dataset.shape))\n",
        "\n",
        "training_set = CustomDataset(train_dataset, tokenizer_statement, MAX_LEN)\n",
        "testing_set = CustomDataset(test_dataset, tokenizer_statement, MAX_LEN)\n",
        "valid_set= CustomDataset(valid_dataset, tokenizer_statement, MAX_LEN)\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': 8,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n",
        "valid_loader=DataLoader(valid_set,**test_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3VAoRdbDl5F",
        "outputId": "afd267d0-bf0c-453f-f8c7-12999a76150c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataset: (10232, 37)\n",
            "TRAIN Dataset: (10232, 37)\n",
            "TEST Dataset: (1264, 36)\n",
            "VALID Dataset: (1280, 37)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of BERT-base to get the final output for the model.\n",
        "from torch import nn\n",
        "\n",
        "class BERT_cnn_Class(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT_cnn_Class, self).__init__()\n",
        "\n",
        "        self.bert_statement = bert_model_statement\n",
        "        self.bert_subject = bert_model_subject\n",
        "        self.bert_context = bert_model_context\n",
        "        self.bert_speaker_job = bert_model_speaker_job\n",
        "        self.bert_party_affiliation = bert_model_party_affiliation\n",
        "\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.activation=torch.nn.ReLU()\n",
        "        self.l3=torch.nn.Conv1d(1, 50, kernel_size=20,stride=1)\n",
        "        self.l4=torch.nn.Conv1d(50,100, kernel_size=20, stride=1)\n",
        "        self.max_pooling=nn.MaxPool1d(2)\n",
        "        self.l5=torch.nn.Linear(17711, 768)\n",
        "        self.l6 = torch.nn.Linear(768, 2)\n",
        "\n",
        "    def forward(self, ids_statement, mask_statement, token_type_ids_statement,\n",
        "                ids_subject, mask_subject, token_type_ids_subject,\n",
        "                ids_context, mask_context, token_type_ids_context,\n",
        "                ids_speaker_job, mask_speaker_job, token_type_ids_speaker_job,\n",
        "                ids_party_affiliation, mask_party_affiliation, token_type_ids_party_affiliation, emotion):\n",
        "\n",
        "        _, output_bert_statement = self.bert_statement(ids_statement, attention_mask = mask_statement, token_type_ids = token_type_ids_statement)\n",
        "        _, output_bert_subject = self.bert_subject(ids_subject, attention_mask = mask_subject, token_type_ids = token_type_ids_subject)\n",
        "        _, output_bert_context = self.bert_context(ids_context, attention_mask = mask_context, token_type_ids = token_type_ids_context)\n",
        "        _, output_bert_speaker_job = self.bert_speaker_job(ids_speaker_job, attention_mask = mask_speaker_job, token_type_ids = token_type_ids_speaker_job)\n",
        "        _, output_bert_party_affiliation = self.bert_party_affiliation(ids_party_affiliation, attention_mask = mask_party_affiliation, token_type_ids = token_type_ids_party_affiliation)\n",
        "\n",
        "        # Remove CLS token from the outputs\n",
        "        output_bert_statement = output_bert_statement[:, 1, :]  # Assuming CLS token is at index 1\n",
        "        output_bert_subject = output_bert_subject[:, 1, :]\n",
        "        output_bert_context = output_bert_context[:, 1, :]\n",
        "        output_bert_speaker_job = output_bert_speaker_job[:, 1, :]\n",
        "        output_bert_party_affiliation = output_bert_party_affiliation[:, 1, :]\n",
        "\n",
        "        output_bert_statement = self.l2(output_bert_statement) #output from bert\n",
        "        output_bert_subject = self.l2(output_bert_subject)\n",
        "        output_bert_context = self.l2(output_bert_context)\n",
        "        output_bert_speaker_job = self.l2(output_bert_speaker_job)\n",
        "        output_bert_party_affiliation = self.l2(output_bert_party_affiliation)\n",
        "\n",
        "        #concat all bert output\n",
        "        concatenated = torch.cat((output_bert_statement, output_bert_subject, output_bert_context, output_bert_speaker_job, output_bert_party_affiliation), dim=1)\n",
        "        concatenated = concatenated.unsqueeze(1)  # Add a channel dimension\n",
        "\n",
        "        #feed into conv net\n",
        "        #first change the size to [8,1,768]\n",
        "        #output_2=torch.cat((emotion,output_2),1) # concat the output of BERT with EMO+SPC+SEN\n",
        "        # output_2=output_2.unsqueeze(1)\n",
        "        # print(output_2.size(),emotion.size())\n",
        "        output_3=self.l3(concatenated)\n",
        "        # print(output_3.size())\n",
        "        output_3=self.activation(output_3)\n",
        "        output_3= self.max_pooling(output_3)\n",
        "        # print(output_3.size())\n",
        "        output_4=self.l4(output_3)\n",
        "        output_4=self.activation(output_4)\n",
        "        # print(output_4.size())\n",
        "        output_4=self.max_pooling(output_4)\n",
        "        # print(output_4.size())\n",
        "        output_4=output_4.view(8,-1)\n",
        "        output_4=torch.cat((emotion,output_4),1) # concat the output of BERT with EMO+SPC+SEN\n",
        "\n",
        "        #change the shape to fit into linear function\n",
        "        #output_4=output_4.view(8,-1)\n",
        "        output_5=self.l5(output_4)\n",
        "        output_5=self.activation(output_5)\n",
        "        output_6=self.l6(output_5)\n",
        "        return output_6\n",
        "\n",
        "model = BERT_cnn_Class()"
      ],
      "metadata": {
        "id": "u58eYihUDn9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2enq-9xDqMQ",
        "outputId": "abcd4049-d376-499f-9900-6d9e3b92fa08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERT_cnn_Class(\n",
              "  (bert_statement): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (bert_subject): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (bert_context): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (bert_speaker_job): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (bert_party_affiliation): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.3, inplace=False)\n",
              "  (activation): ReLU()\n",
              "  (l3): Conv1d(1, 50, kernel_size=(20,), stride=(1,))\n",
              "  (l4): Conv1d(50, 100, kernel_size=(20,), stride=(1,))\n",
              "  (max_pooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (l5): Linear(in_features=17711, out_features=768, bias=True)\n",
              "  (l6): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "PIQY5uKxDs_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "21fRCqB9DtkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAIN THE MODEL\n",
        "val_losses=[]\n",
        "train_losses=[]\n",
        "accuracy_list=[]\n",
        "\n",
        "for epoch in range(2):\n",
        "  t0 = time.time()\n",
        "  model.train()\n",
        "  print(f\"\\t Epoch: {epoch}  is Started: \")\n",
        "  batch=0\n",
        "  train_loss=0\n",
        "\n",
        "  for _,data in enumerate(training_loader, 0):\n",
        "      try:\n",
        "          ids_statement = data['ids_statement'].to(device, dtype = torch.long)\n",
        "          mask_statement = data['mask_statement'].to(device, dtype = torch.long)\n",
        "          token_type_ids_statement = data['token_type_ids_statement'].to(device, dtype = torch.long)\n",
        "          ids_subject = data['ids_subject'].to(device, dtype = torch.long)\n",
        "          mask_subject = data['mask_subject'].to(device, dtype = torch.long)\n",
        "          token_type_ids_subject = data['token_type_ids_subject'].to(device, dtype = torch.long)\n",
        "          ids_context = data['ids_context'].to(device, dtype = torch.long)\n",
        "          mask_context = data['mask_context'].to(device, dtype = torch.long)\n",
        "          token_type_ids_context = data['token_type_ids_context'].to(device, dtype = torch.long)\n",
        "          ids_speaker_job = data['ids_speaker_job'].to(device, dtype = torch.long)\n",
        "          mask_speaker_job = data['mask_speaker_job'].to(device, dtype = torch.long)\n",
        "          token_type_ids_speaker_job = data['token_type_ids_speaker_job'].to(device, dtype = torch.long)\n",
        "          ids_party_affiliation = data['ids_party_affiliation'].to(device, dtype = torch.long)\n",
        "          mask_party_affiliation = data['mask_party_affiliation'].to(device, dtype = torch.long)\n",
        "          token_type_ids_party_affiliation = data['token_type_ids_party_affiliation'].to(device, dtype = torch.long)\n",
        "\n",
        "          targets = data['targets'].to(device, dtype = torch.float)\n",
        "          emotions=data['emotion'].to(device,dtype=torch.float)\n",
        "      except:\n",
        "          print(f\"some error at testing {batch}\")\n",
        "          print(data['dfID'])\n",
        "\n",
        "      try:\n",
        "        outputs = model(ids, mask, token_type_ids, emotions)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        train_loss+=loss.item()\n",
        "        #print(f'{count} Loss:  {loss.item()}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch+=1\n",
        "      except EOFError:\n",
        "        print(f\"{data['dfID']} error because of batch size-------->\", EOFError)\n",
        "        print(f\"some error at testing {batch}\")\n",
        "        print(data['dfID'] )\n",
        "  print(f\"   Epoch: {epoch} Train loss is :{train_loss/batch}\")\n",
        "  train_loss /=batch\n",
        "  train_losses.append(train_loss)\n",
        "  print(f\"   Epoch {epoch} took: {format_time(time.time() - t0)} \\n\")\n",
        "\n",
        "  model.eval()\n",
        "  fin_targets=[]\n",
        "  fin_outputs=[]\n",
        "  with torch.no_grad():\n",
        "      val_loss, batch = 0, 1\n",
        "      for _, data in enumerate(testing_loader, 0):\n",
        "          ids = data['ids'].to(device, dtype = torch.long)\n",
        "          mask = data['mask'].to(device, dtype = torch.long)\n",
        "          token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "          targets = data['targets'].to(device, dtype = torch.float)\n",
        "          emotions=data['emotion'].to(device,dtype=torch.float)\n",
        "          batch+=1\n",
        "          try:\n",
        "                        outputs = model(ids, mask, token_type_ids,emotions)\n",
        "                        loss = loss_fn(outputs, targets)\n",
        "                        val_loss+=loss.item()\n",
        "                        fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "                        fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "          except:\n",
        "            print(f\"some error at testing {batch}\")\n",
        "            print(data['dfID'] )\n",
        "\n",
        "      val_loss/=batch\n",
        "      val_losses.append(val_loss)\n",
        "  outputs=fin_outputs\n",
        "  outputs = np.array(outputs) >= 0.5\n",
        "  targets=fin_targets\n",
        "  accuracy = metrics.accuracy_score(targets, outputs)\n",
        "  accuracy_list.append(accuracy)\n",
        "  f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
        "  f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "  recall_micro = metrics.recall_score(targets, outputs, average='micro')\n",
        "  recall_macro = metrics.recall_score(targets, outputs, average='macro')\n",
        "  precision_micro = metrics.precision_score(targets, outputs, average='micro')\n",
        "  precision_macro = metrics.precision_score(targets, outputs, average='macro')\n",
        "  print(f\"Epoch: {epoch} - Accuracy on Testing Data Score = {accuracy}\")\n",
        "  print(f\"Epoch: {epoch} - F1 Score on Testing Data (Micro) = {f1_score_micro}\")\n",
        "  print(f\"Epoch: {epoch} - F1 Score on Testing Data (Macro) = {f1_score_macro}\")\n",
        "  print(f\"Epoch: {epoch} - recall Score on Testing Data (Micro) = {recall_micro}\")\n",
        "  print(f\"Epoch: {epoch} - recall Score on Testing Data (Macro) = {recall_macro}\")\n",
        "  print(f\"Epoch: {epoch} - precision Score on Testing Data (Micro) = {precision_micro}\")\n",
        "  print(f\"Epoch: {epoch} - precision Score on Testing Data (Macro) = {precision_macro}\")\n",
        "  print(f\"\\n \\t Epoch {epoch} : Train Loss (Training Data):{train_loss}, Validation Loss (Testing Data): {val_loss}\")\n",
        "  print(\"_________________________________________________\\n\")\n",
        "  #if train_loss > val_loss:\n",
        "  # torch.save(model.state_dict(), \"/content/drive/My Drive/Bibek/models_saved/w9p7\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "TVpu3-u5Dwpo",
        "outputId": "5c0e2432-0b1a-4484-b2a5-beef93d732e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t Epoch: 0  is Started: \n",
            "some error at testing 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-558bc3484a47>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m           \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m           \u001b[0memotions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emotion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'targets'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-558bc3484a47>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"some error at testing {batch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dfID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'dfID'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import DataFrame\n",
        "df=DataFrame(train_losses,columns=['train_losses'])\n",
        "df=DataFrame(val_losses,columns=['val_losses'])\n",
        "df.to_csv(\"/content/drive/My Drive/SentimentalLIAR/Results/w10_p4-1031.csv\")"
      ],
      "metadata": {
        "id": "kZkaFpp0D6La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses, label=\"Training loss\")\n",
        "plt.plot(val_losses, label=\"Validation loss\")\n",
        "\n",
        "plt.xlabel('Epoch', fontsize=18)\n",
        "plt.ylabel('Losses', fontsize=16)\n",
        "plt.title('Training Loss VS Validation Loss', fontsize=15)\n",
        "\n",
        "plt.legend()\n",
        "plt.savefig('/content/drive/My Drive/SentimentalLIAR/Results/w10_p4-epoch1-1031.eps')\n",
        "\n",
        "#plt.title(\"Losses\")"
      ],
      "metadata": {
        "id": "X3Dfz2ooD9Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test on validation data\n",
        "model.eval()\n",
        "fin_targets=[]\n",
        "fin_outputs=[]\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    c=0\n",
        "    for _, data in enumerate(valid_loader, 0):\n",
        "        ids_statement = data['ids_statement'].to(device, dtype = torch.long)\n",
        "        mask_statement = data['mask_statement'].to(device, dtype = torch.long)\n",
        "        token_type_ids_statement = data['token_type_ids_statement'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        emotions=data['emotion'].to(device,dtype=torch.float)\n",
        "        c+=1\n",
        "        try:\n",
        "                      outputs = model(ids, mask, token_type_ids,emotions)\n",
        "                      fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "                      fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "        except EOFError:\n",
        "          print(f\"some error at {c}\",EOFError)\n",
        "outputs=fin_outputs\n",
        "outputs = np.array(outputs) >= 0.5\n",
        "targets=fin_targets\n",
        "accuracy = metrics.accuracy_score(targets, outputs)\n",
        "\n",
        "f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
        "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "recall_micro = metrics.recall_score(targets, outputs, average='micro')\n",
        "recall_macro = metrics.recall_score(targets, outputs, average='macro')\n",
        "precision_micro = metrics.precision_score(targets, outputs, average='micro')\n",
        "precision_macro = metrics.precision_score(targets, outputs, average='macro')\n",
        "print(f\"Epoch: {epoch} - Accuracy Score on validation data = {accuracy}\")\n",
        "print(f\"Epoch: {epoch} - F1 Score on Validation Data (Micro) = {f1_score_micro}\")\n",
        "print(f\"Epoch: {epoch} - F1 Score on Validation Data (Macro) = {f1_score_macro}\")\n",
        "print(f\"Epoch: {epoch} - recall Score on Validation Data (Micro) = {recall_micro}\")\n",
        "print(f\"Epoch: {epoch} - recall Score on Validation Data (Macro) = {recall_macro}\")\n",
        "print(f\"Epoch: {epoch} - precision Score on Validation Data (Micro) = {precision_micro}\")\n",
        "print(f\"Epoch: {epoch} - precision Score on Validation Data (Macro) = {precision_macro}\")\n",
        "\n",
        "print(\"____________________________________________________________\\n________________________________________________________________\\n\\n\")"
      ],
      "metadata": {
        "id": "YSHvMhVyECFm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}